{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [last week's blog](http://qingkaikong.blogspot.com/2016/11/machine-learning-3-artificial-neural.html), we talked about the basics of Artificial Neural Network (ANN), and gave an intuitive summary and example. Hope you already get a sense of how ANN works. This week, we will continue to explore more of the ANN, and implement a simple version of it so that you will have a better understanding.  \n",
    "\n",
    "Key concepts you will get from this blog: Inputs, Weights, Outputs, Targets, Activation Function, Error, Bias input, Learning rate.   \n",
    "\n",
    "Let's start with the simple [Perceptron](https://en.wikipedia.org/wiki/Perceptron), which developed back to the late 1950s; its first implementation, in custom hardware, was one of the first artificial neural networks to be produced. You can also treat it as a two layer neural network without the hidden layers. Even though it has limitations, it contains most of the parts we want to cover for the ANN.   \n",
    "\n",
    "Let's see this example with 4 data samples, this is the input of the data. Each of the sample has 3 features. The target of the data is simplely 0 or 1. We want to build a simple Perceptron model that can output the correct target by feeding into the 3 features from the data samples. See the following table. This example is modified from this [great post](http://iamtrask.github.io/2015/07/12/basic-python-network/).    \n",
    "\n",
    "|Feature1|Feature2|Feature3|Target|\n",
    "|:------:|:------:|:------:|:----:|\n",
    "|    0   |    0   |    1   |   0  |\n",
    "|    1   |    1   |    1   |   1  |\n",
    "|    1   |    0   |    1   |   1  |\n",
    "|    0   |    1   |    1   |   0  |\n",
    "\n",
    "Let's have a look of the structure of our model.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/qingkaikong/blog/master/39_ANN_part2_step_by_step/figures/figure1_perceptron_structure.jpg\" width=\"600\"/>  \n",
    "\n",
    "We can see from the figure that we have two layers in this model: the input layer and the output layer. For the input layer, it has 3 features that connect to the output layer via weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00966449]\n",
      " [ 0.99211957]\n",
      " [ 0.99358898]\n",
      " [ 0.00786506]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The activation function, we will use the sigmoid\n",
    "\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# input dataset\n",
    "X = np.array([[0,0,1],[1,1,1],[1,0,1],[0,1,1]])\n",
    "\n",
    "# output dataset           \n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "weights_0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in xrange(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    layer_0 = X\n",
    "\n",
    "    layer_1 = sigmoid(np.dot(layer_0,weights_0))\n",
    "\n",
    "    # how much difference? This will be the error of \n",
    "    # our estimation and the true value\n",
    "    layer1_error = y - layer_1\n",
    "\n",
    "    # multiply how much we missed by the\n",
    "    # slope of the sigmoid at the values in l1\n",
    "    layer1_delta = layer1_error * sigmoid(layer_1,True)\n",
    "\n",
    "    # update weights\n",
    "    weights_0 += np.dot(layer_0.T,layer1_delta)\n",
    "print \"Output After Training:\"\n",
    "print l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References  \n",
    "\n",
    "[A Neural Network in 11 lines of Python](http://iamtrask.github.io/2015/07/12/basic-python-network/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
