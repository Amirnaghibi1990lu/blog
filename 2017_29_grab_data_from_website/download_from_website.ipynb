{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to grab data from a website, where data is stored in each day (different pages). Python is really a good tool to do that for many pages instead you go through them one by one copy it. This week, one friend asked me about exactly the problem and here is how we did it. \n",
    "\n",
    "## The problem\n",
    "\n",
    "We want to download data from [this website](https://www.wunderground.com/history/airport/KICT/,2017/1/1/DailyHistory.html?req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=), the **Sea Level Pressure** value from the webpage and save it to the csv file, for example, the red part in the following figure:\n",
    "\n",
    "![png](./figures/Sea_pressure_one_value.png)\n",
    "\n",
    "## Grab the value from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import datetime\n",
    "import numpy as np\n",
    "from BeautifulSoup import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define a function that takes in year, month and day, and generate the url for the website for that day. If you look at the website url, for example: 'https://www.wunderground.com/history/airport/KICT/2017/1/1/DailyHistory.html?req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=', this website is for 2017-01-01, it is easier to think the website is like in the folders, the path to the folder is this link, if we want data from 2017-01-02, we only need to change it to 'https://www.wunderground.com/history/airport/KICT/2017/1/2/DailyHistory.html?req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo='. Now you understand why we will pass the year, month, day to the function to generate the url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_url(year, month, day):\n",
    "    base_url = 'https://www.wunderground.com/history/airport/KICT/%d/%d/%d/DailyHistory.html?&reqdb.zip=&reqdb.magic=&reqdb.wmo='%(year, month, day)\n",
    "    return base_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate a list of dates from the starting date to the ending date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 2, 0, 0), datetime.datetime(2017, 1, 3, 0, 0), datetime.datetime(2017, 1, 4, 0, 0), datetime.datetime(2017, 1, 5, 0, 0), datetime.datetime(2017, 1, 6, 0, 0), datetime.datetime(2017, 1, 7, 0, 0), datetime.datetime(2017, 1, 8, 0, 0), datetime.datetime(2017, 1, 9, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.strptime(\"2017-01-01\", \"%Y-%m-%d\")\n",
    "end = datetime.datetime.strptime(\"2017-01-10\", \"%Y-%m-%d\")\n",
    "date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\n",
    "print(date_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is the fun part, we will use the [urllib module](https://docs.python.org/2/library/urllib2.html) to open the url and use the [BeautifulSoup module](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to parse the html webpage. Let's first do this for one page: 2017-01-01.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.wunderground.com/history/airport/KICT/2017/1/1/DailyHistory.html?req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo='\n",
    "\n",
    "response = urllib2.urlopen(url)\n",
    "soup = BeautifulSoup(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we can extract the information is that we check with the page source code - html in this case, and find the information we want. Go to the [webpage](https://www.wunderground.com/history/airport/KICT/2017/1/1/DailyHistory.html?req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=), and right click on the page, and choose 'View Page Source'. You will see the raw html for the webpage. Browse through the source page, you will see find that the data we want stored in a table, see the following figure. \n",
    "\n",
    "![png](./figures/html_source.png)\n",
    "\n",
    "The red rectangle highlight the part we want, an HTML table is defined with the $<table>$ tag. Each table row is defined with the $<tr>$ tag. A table header is defined with the $<th>$ tag. By default, table headings are bold and centered. A table data/cell is defined with the $<td>$ tag. You can find more details [here](https://www.w3schools.com/html/html_tables.asp). \n",
    "\n",
    "Now, we have an idea how the data is structured, and how we can extract it: we first find the table we want (there are multiple tables in the page). Then we find all the rows in the table, and loop through it see if it contains the 'Sea Level Pressure', if it does, we then extract the $<span class>$ with an value \"wx-value\". By browsing through the html, we find the data we want stored in the table with a class 'responsive airport-history-summary-table' or id 'historyTable', which we can use this information to search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = soup.find('table', {'id': 'historyTable'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print out the table content and see what is inside (you will see the raw html as the image showing above). We then loop through each row of this table and find the row which contains 'Sea Level Pressure':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.85\n"
     ]
    }
   ],
   "source": [
    "# we loop through each row\n",
    "for row in table.findAll('tr'):\n",
    "    # find all the cells in the row\n",
    "    data = row.findAll('td')\n",
    "    \n",
    "    # check if the 'Sea Level Pressure'\n",
    "    if data and 'Sea Level Pressure' in data[0].text:\n",
    "\n",
    "        if data[1].find('span'):\n",
    "            pressure = data[1].find('span', attrs={'class' : 'wx-value'}).text\n",
    "            \n",
    "print pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code starts with table.findAll('tr'), which basically saying we want to find all the rows in this table and return them in a list. Then we loop through the each row, and find all the cells in the row, that is row.findAll('td'). The if statement checks whether the first cell in the data contains \"Sea Level Pressure\" as the title or not, if it does, we will find the next cell which contains a span class with 'wx-value' in it, that is the value we want. Also you notitce that I have another if statement to check *if data[1].find('span')*, we want to see if the cell contains span or not, you will see that some of the cell contains \"Sea Level Pressure\" but no span (that is the title in the table). \n",
    "\n",
    "Anyway, we successfully extract the pressure value from the table, and let's put everything together by defining a function to return the pressure from an url, and then we can loop through all the dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pressure(url):\n",
    "    response = urllib2.urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    \n",
    "    table = soup.find('table', {'id': 'historyTable'})\n",
    "    \n",
    "    for row in table.findAll('tr'):\n",
    "        data = row.findAll('td')\n",
    "        if data and 'Sea Level Pressure' in data[0].text:\n",
    "            \n",
    "            if data[1].find('span'):\n",
    "                pressure = data[1].find('span', attrs={'class' : 'wx-value'}).text\n",
    "                \n",
    "    return pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2017-01-01', 29.85], ['2017-01-02', 29.78], ['2017-01-03', 30.18], ['2017-01-04', 30.24], ['2017-01-05', 30.24], ['2017-01-06', 30.43], ['2017-01-07', 30.65], ['2017-01-08', 30.43], ['2017-01-09', 29.97]]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for date in date_generated:\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    # generate the url for each day\n",
    "    url = generate_url(year, month, day) \n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    pressure = get_pressure(url)\n",
    "    results.append([date_str, float(pressure)]) \n",
    "\n",
    "print(results)\n",
    "# you can also save it as csv\n",
    "#np.savetxt('pressure.csv', np.array(results), fmt = '%s', delimiter=',', header='date, pressure(in)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the table values\n",
    "\n",
    "Ok, till now, I think you already known how to extract values from a cell in a table. Let's do another test: If you scroll down two the bottow of the [page](https://www.wunderground.com/history/airport/KICT/2017/1/1/DailyHistory.html?req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=), you will see a table with about each hour measurements (see the following figure), now we want to get each day, each hour sea level pressure measurements. One example is:\n",
    "\n",
    "![png](./figures/Sea_pressure_table.png)\n",
    "\n",
    "I think I don't need to explain more, see the code below, you can figure it out :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01 12:53 AM 29.99\n",
      "2017-01-01 1:53 AM 29.97\n",
      "2017-01-01 2:53 AM 29.94\n",
      "2017-01-01 3:53 AM 29.95\n",
      "2017-01-01 4:53 AM 29.93\n",
      "2017-01-01 5:53 AM 29.95\n",
      "2017-01-01 6:53 AM 29.96\n",
      "2017-01-01 7:53 AM 29.96\n",
      "2017-01-01 8:53 AM 29.96\n",
      "2017-01-01 9:53 AM 29.94\n",
      "2017-01-01 10:53 AM 29.92\n",
      "2017-01-01 11:53 AM 29.89\n",
      "2017-01-01 12:53 PM 29.84\n",
      "2017-01-01 1:31 PM 29.80\n",
      "2017-01-01 1:53 PM 29.81\n",
      "2017-01-01 2:41 PM 29.77\n",
      "2017-01-01 2:53 PM 29.77\n",
      "2017-01-01 3:53 PM 29.77\n",
      "2017-01-01 4:53 PM 29.76\n",
      "2017-01-01 5:53 PM 29.77\n",
      "2017-01-01 6:53 PM 29.77\n",
      "2017-01-01 7:53 PM 29.78\n",
      "2017-01-01 8:53 PM 29.79\n",
      "2017-01-01 9:13 PM 29.77\n",
      "2017-01-01 9:53 PM 29.78\n",
      "2017-01-01 10:00 PM 29.77\n",
      "2017-01-01 10:46 PM 29.77\n",
      "2017-01-01 10:53 PM 29.78\n",
      "2017-01-01 11:37 PM 29.77\n",
      "2017-01-01 11:53 PM 29.78\n",
      "2017-01-02 12:20 AM 29.76\n",
      "2017-01-02 12:53 AM 29.76\n",
      "2017-01-02 1:53 AM 29.75\n",
      "2017-01-02 2:53 AM 29.76\n",
      "2017-01-02 3:38 AM 29.75\n",
      "2017-01-02 3:53 AM 29.75\n",
      "2017-01-02 4:53 AM 29.73\n",
      "2017-01-02 5:32 AM 29.72\n",
      "2017-01-02 5:53 AM 29.71\n",
      "2017-01-02 6:18 AM 29.72\n",
      "2017-01-02 6:53 AM 29.73\n",
      "2017-01-02 7:53 AM 29.76\n",
      "2017-01-02 8:42 AM 29.75\n",
      "2017-01-02 8:53 AM 29.76\n",
      "2017-01-02 9:06 AM 29.74\n",
      "2017-01-02 9:43 AM 29.75\n",
      "2017-01-02 9:53 AM 29.77\n",
      "2017-01-02 10:51 AM 29.76\n",
      "2017-01-02 10:53 AM 29.76\n",
      "2017-01-02 11:16 AM 29.77\n",
      "2017-01-02 11:36 AM 29.75\n",
      "2017-01-02 11:53 AM 29.75\n",
      "2017-01-02 12:53 PM 29.73\n",
      "2017-01-02 1:53 PM 29.71\n",
      "2017-01-02 2:53 PM 29.71\n",
      "2017-01-02 3:26 PM 29.71\n",
      "2017-01-02 3:53 PM 29.73\n",
      "2017-01-02 4:53 PM 29.75\n",
      "2017-01-02 5:53 PM 29.77\n",
      "2017-01-02 6:51 PM 29.80\n",
      "2017-01-02 6:53 PM 29.80\n",
      "2017-01-02 7:08 PM 29.81\n",
      "2017-01-02 7:20 PM 29.82\n",
      "2017-01-02 7:29 PM 29.83\n",
      "2017-01-02 7:53 PM 29.83\n",
      "2017-01-02 8:11 PM 29.85\n",
      "2017-01-02 8:25 PM 29.86\n",
      "2017-01-02 8:53 PM 29.88\n",
      "2017-01-02 9:48 PM 29.89\n",
      "2017-01-02 9:53 PM 29.89\n",
      "2017-01-02 10:43 PM 29.90\n",
      "2017-01-02 10:53 PM 29.91\n",
      "2017-01-02 11:18 PM 29.90\n",
      "2017-01-02 11:53 PM 29.91\n"
     ]
    }
   ],
   "source": [
    "# To save space, let's only get two days data\n",
    "start = datetime.datetime.strptime(\"2017-01-01\", \"%Y-%m-%d\")\n",
    "end = datetime.datetime.strptime(\"2017-01-03\", \"%Y-%m-%d\")\n",
    "date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\n",
    "\n",
    "results = []\n",
    "for date in date_generated:\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    # generate the url for each day\n",
    "    url = generate_url(year, month, day) \n",
    "    # keep a string for that date\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    response = urllib2.urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    \n",
    "    # by looking at the html, we found the table name is 'obs-table responsive'\n",
    "    table = soup.find('table', {'class': 'obs-table responsive'})\n",
    "    \n",
    "    # loop through the table row without the first row (that is the header)\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        # find all the cells\n",
    "        cells = row.findAll('td')\n",
    "        \n",
    "        # we can see easily the time is in the first cell, and the pressure value is in the 5th cell\n",
    "        results.append([date_str, cells[0].text, cells[5].find('span', attrs={'class' : 'wx-value'}).text])\n",
    "\n",
    "print(\"\\n\".join(' '.join(map(str,sl)) for sl in results))      \n",
    "#np.savetxt('pressure.csv', np.array(results), fmt = '%s', delimiter=',', header='date, time (CST), pressure (in)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
